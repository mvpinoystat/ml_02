{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bef5c308-3bda-41b1-b71b-fe2f7159023a",
   "metadata": {},
   "source": [
    "# **Modeling**\n",
    "Submitted Jan 3, 2024\n",
    "public leaderboard score: 0.656 V5  \n",
    "pulbic leaderboard score: 0.653 V6   \n",
    "public leaderboard score: 0.683 V8   \n",
    "public leaderboard score: 0.676 V9   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f4c398f-e96b-4315-bed0-51a28628b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler, SplineTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#model selection\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "#load preprocessed dataset:\n",
    "import joblib\n",
    "#models\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa04a126-e352-42a2-b6c8-e1dee66c0c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"kaggle/input/linking-writing-processes-to-writing-quality/\"\n",
    "train_logs = pd.read_csv(input_folder + \"train_logs.csv\",delimiter = \",\",header = 0)\n",
    "train_scores = pd.read_csv(input_folder +\"train_scores.csv\", delimiter = \",\", header = 0)\n",
    "scores = pd.Series(data = train_scores['score'].values, index = train_scores['id'].values, name = 'score')\n",
    "test_logs = pd.read_csv(input_folder + \"test_logs.csv\",delimiter = \",\",header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bfd31e3-a52c-4fa9-9fad-88fc1194092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for transformer for cursor position\n",
    "class CursorPositionTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # setup the feature names\n",
    "        # self.feature_names = ['cp_sum_backstep', 'cp_n_backstep', 'cp_sum_forwardstep','cp_n_forwardstep',\n",
    "        #              'cp_change_stat', 'cp_skew_backstep', 'cp_skew_forwardstep']  \n",
    "\n",
    "        self.feature_names = ['cp_sum_backstep', 'cp_n_backstep', 'cp_sum_forwardstep','cp_n_forwardstep']\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        z = X.groupby('id')['cursor_position'].aggregate([self.cp_sum_backstep,self.cp_n_backstep, \n",
    "                     self.cp_sum_forwardstep, self.cp_n_forwardstep])\n",
    "        # make a copy of participant ids:\n",
    "        self.index_ids = z.index.values\n",
    "        return z.values\n",
    "\n",
    "    def cp_sum_backstep(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return np.sum(n1[n1 < 0])\n",
    "    \n",
    "    def cp_skew_backstep(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return st.skew(n1[n1 < 0])\n",
    "    \n",
    "    def cp_n_backstep(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return np.log((n1<0).sum()+1)\n",
    "    \n",
    "    def cp_sum_forwardstep(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return np.sum(n1[n1 > 0])\n",
    "    \n",
    "    def cp_skew_forwardstep(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return st.skew(n1[n1 > 0])\n",
    "    \n",
    "    def cp_n_forwardstep(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return np.log((n1>0).sum()+1)\n",
    "    \n",
    "    def cp_change_stat(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return np.std(n1, ddof = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa1c7d61-6548-4353-a292-d7bae2300246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda wordcount transformer:\n",
    "\n",
    "# word_count feature engineering\n",
    "# Based on the graph above, we can count the number of zero changes and get the mean:\n",
    "# wc_zero_change will return the count of all non-zero steps taken by the person\n",
    "\n",
    "class WordCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def wc_non_zero_change(self, x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        n2 = np.count_nonzero(n1)\n",
    "        return n2\n",
    "    def wc_change_stat(self, x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        last_cutoff = n1.shape[0]-200\n",
    "        n2 = np.std(n1, ddof = 1)\n",
    "        return n2\n",
    "        \n",
    "    def transform(self, X):\n",
    "        output =  X.groupby(['id'])['word_count'].aggregate([self.wc_non_zero_change,lambda x: np.log(len(x)), \n",
    "                          lambda x: np.log(np.max(x)+1)])\n",
    "        output.columns = [\"wc_changing_nsteps\", \"wc_step_count\", \"wc_max\"]\n",
    "        self.feature_names = output.columns.values\n",
    "        self.index_ids = output.index.values\n",
    "        return output.values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7645e712-9d3e-40d4-aa02-eee5a2f18294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda textchange transformer:\n",
    "class TextChangeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def hasChar(self, x,character:str):\n",
    "        out = 0 \n",
    "        for strings in x:\n",
    "            if character in strings:\n",
    "                out = 1\n",
    "                break\n",
    "        return out\n",
    "        \n",
    "    def transform(self, X):\n",
    "        output = X.groupby(['id'])['text_change'].aggregate([\n",
    "            (\"tc_1\", lambda x: self.hasChar(x,character = \"?\")), \n",
    "            (\"tc_2\", lambda x: self.hasChar(x,character = \"=>\")), \n",
    "            (\"tc_3\", lambda x: self.hasChar(x,character = \"(\")), \n",
    "            (\"tc_4\", lambda x: self.hasChar(x,character = \"\\\"\")), \n",
    "            (\"tc_5\", lambda x: self.hasChar(x,character = \"-\"))]) \n",
    "        self.feature_names = output.columns.values\n",
    "        self.index_ids = output.index.values\n",
    "        return output.values\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0542b309-5aed-46c2-b33b-1c6b0931372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Up Event Variable Transformer:\n",
    "class UpEventTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "\n",
    "    \n",
    "    def find_clicked(self, x, st:str):\n",
    "        has_string = 0\n",
    "        for event in x:\n",
    "            if(event == st):\n",
    "                has_string = 1\n",
    "                break\n",
    "        return has_string\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        output = X.groupby(['id'])['up_event'].aggregate([('ue_1',lambda x: self.find_clicked(x,\"|\")),\n",
    "                                                          ('ue_2', lambda x: self.find_clicked(x,\"Shift\")),\n",
    "                                                          ('ue_3', lambda x: self.find_clicked(x,\"Tab\")),\n",
    "                                                          ])\n",
    "        self.feature_names = output.columns.values\n",
    "        self.index_ids = output.index.values\n",
    "        return output.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0acf834e-2426-429a-b6e2-db64d907af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eda action_time variable ransformer: (AT)\n",
    "\n",
    "class ActionTimeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, scores):\n",
    "        self.scores = scores\n",
    "        self.score_values = np.arange(start = 0.5, stop = 6.5, step = 0.5)\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        #Get the action time proportion or distribution per score:\n",
    "        at_init = X.groupby('id')['action_time'].aggregate([\n",
    "            ('one', lambda x: self.above_log_count(x, from_zero = 1)),\n",
    "            ('two', lambda x: self.above_log_count(x, from_zero = 2)),\n",
    "            ('three', lambda x: self.above_log_count(x, from_zero = 3)),\n",
    "            ('four', lambda x: self.above_log_count(x, from_zero = 4)),\n",
    "            ('five', lambda x: self.above_log_count(x, from_zero = 5)),\n",
    "        ])\n",
    "        \n",
    "        at_init2 = pd.merge(at_init, self.scores, left_index = True, right_index = True)\n",
    "        at2 = at_init2.groupby(by = 'score').sum()\n",
    "        self.at_proportion= at2.apply(lambda x: x/(np.sum(at2, axis = 1)))\n",
    "        return self\n",
    "        \n",
    "    def above_log_count(self, x, from_zero = 1):\n",
    "        z = np.diff(np.log(x+1))\n",
    "        z = np.abs(z)\n",
    "        if from_zero < 5:\n",
    "            count= len(list(filter(lambda q: (q>from_zero -1) and (q < from_zero), z)))\n",
    "        else:\n",
    "            count= len(list(filter(lambda q: q>=from_zero, z )))\n",
    "        return count \n",
    "        \n",
    "    def above_log_ratio(self, x, from_zero = 1):\n",
    "        z = np.diff(np.log(x+1))\n",
    "        z = np.abs(z)\n",
    "        if from_zero < 3:\n",
    "            count= len(list(filter(lambda q: (q>from_zero -1) and (q < from_zero), z)))\n",
    "        else:\n",
    "            count= len(list(filter(lambda q: q>=from_zero, z )))\n",
    "        return np.log((count+1)/len(z)) \n",
    "\n",
    "        \n",
    "    # Use chi-square to select the score of the given participant id   \n",
    "    def compute_score_by_chisquare(self, fo:pd.Series, distribution):\n",
    "        fo =fo + 1 # to remove errors for those with zero values\n",
    "        total = np.sum(fo)\n",
    "        # print(total)\n",
    "        expected_arrays = distribution * total\n",
    "        # print(expected_arrays)\n",
    "        chi_stat = []\n",
    "        for j in range(expected_arrays.shape[0]):\n",
    "            results = st.chisquare(f_obs = fo, f_exp = expected_arrays.iloc[j])\n",
    "            chi_stat.append(results[1])\n",
    "    \n",
    "        chi_stat = np.array(chi_stat)\n",
    "        # get the maximum p-value (-1) or second to the max (-2), etc\n",
    "        score_idx_1 = np.where(chi_stat == np.partition(chi_stat,-1)[-1])[0][0]\n",
    "        score_idx_2 = np.where(chi_stat == np.partition(chi_stat,-2)[-2])[0][0]\n",
    "        score_idx_3 = np.where(chi_stat == np.partition(chi_stat,-3)[-3])[0][0]\n",
    "        score_idx_4 = np.where(chi_stat == np.partition(chi_stat,-4)[-4])[0][0]\n",
    "        score_list = [\n",
    "            self.score_values[score_idx_1],\n",
    "            self.score_values[score_idx_3],\n",
    "            self.score_values[score_idx_3],\n",
    "            self.score_values[score_idx_4]]\n",
    "        \n",
    "        return np.mean(score_list)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        transform_1 = X.groupby(\"id\")['action_time'].aggregate([\n",
    "        ('at_1', lambda x: self.above_log_ratio(x, from_zero = 1)),\n",
    "        ('at_2', lambda x: self.above_log_ratio(x, from_zero = 2)),\n",
    "        ('at_3', lambda x: self.above_log_ratio(x, from_zero = 3))\n",
    "        ])\n",
    "        \n",
    "        at_init = X.groupby('id')['action_time'].aggregate([\n",
    "            ('one', lambda x: self.above_log_count(x, from_zero = 1)),\n",
    "            ('two', lambda x: self.above_log_count(x, from_zero = 2)),\n",
    "            ('three', lambda x: self.above_log_count(x, from_zero = 3)),\n",
    "            ('four', lambda x: self.above_log_count(x, from_zero = 4)),\n",
    "            ('five', lambda x: self.above_log_count(x, from_zero = 5)),\n",
    "        ])\n",
    "        transform_2 = at_init.apply(\n",
    "            lambda x: self.compute_score_by_chisquare(x, distribution = self.at_proportion),axis = 1)\n",
    "        transform_2.name = \"at_chisq\"\n",
    "        output = pd.merge(transform_1, transform_2, left_index = True, right_index = True)\n",
    "        self.feature_names = output.columns.values\n",
    "        self.index_ids = output.index.values\n",
    "        return output.values \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94f4d027-ec0f-4db3-aa93-3d398a464359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer for Activity, act:\n",
    "class ActivityTransformer(BaseEstimator, TransformerMixin):\n",
    "    oneHot: OneHotEncoder\n",
    "    scores: pd.Series\n",
    "    act_dist: pd.DataFrame\n",
    "    feature_names: np.array\n",
    "    initial_features: np.array\n",
    "    \n",
    "    def __init__(self, scores:pd.Series):\n",
    "        self.oneHot = OneHotEncoder(handle_unknown = 'ignore', categories = 'auto', sparse_output = False)\n",
    "        self.scores = scores\n",
    "        self.score_values = np.arange(start = 0.5, stop = 6.5, step = 0.5)\n",
    "        self.initial_features = np.array(['ac_Input', 'ac_Move', 'ac_NonPro', 'ac_Paste', 'ac_RemCut', 'ac_Replace'])\n",
    "        \n",
    "    def fit(self,X, y=None):\n",
    "        #Transform X labels first:\n",
    "        #Transform all with move into a Move:\n",
    "        X.activity = X.activity.apply(lambda x: \"Move\" if (\"Move\" in x) else x)\n",
    "        #Encode then get the distribution\n",
    "        self.oneHot.fit(X)\n",
    "        a1 = self.oneHot.fit_transform(X.activity.values.reshape(-1,1))\n",
    "        a2 = pd.DataFrame(data=a1, columns=self.initial_features)\n",
    "        a2['id'] = X.id.copy()\n",
    "        \n",
    "        act = a2.groupby(by = \"id\").sum()\n",
    "        act = act + 1 # to avoid expected value of zero\n",
    "        \n",
    "        # Get the distribution for each kind of score\n",
    "        # act distribution:\n",
    "        act_dist = pd.merge(act, scores, left_index = True, right_index = True)\n",
    "        act_dist = act_dist.groupby('score').sum()\n",
    "        \n",
    "        row_total = np.sum(act_dist, axis = 1)\n",
    "        self.act_dist = act_dist.apply(lambda x: x / row_total)\n",
    "            \n",
    "        return self\n",
    "\n",
    "\n",
    "    def compute_score_by_chisquare(self, fo:pd.Series, distribution):\n",
    "        fo = fo+1\n",
    "        total = np.sum(fo)\n",
    "        # print(total)\n",
    "        # add 1 to avoid expected value of zero.\n",
    "        expected_arrays = distribution * total \n",
    "        # print(expected_arrays)\n",
    "        chi_stat = []\n",
    "        for j in range(expected_arrays.shape[0]):\n",
    "            results = st.chisquare(f_obs = fo, f_exp = expected_arrays.iloc[j])\n",
    "            chi_stat.append(results[1])\n",
    "    \n",
    "        chi_stat = np.array(chi_stat)\n",
    "        # get the maximum p-value (-1) \n",
    "        score_idx_1 = np.where(chi_stat == np.partition(chi_stat,-1)[-1])[0][0]\n",
    "        \n",
    "        return self.score_values[score_idx_1]\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        #Transform X labels first:\n",
    "        #Transform all with move into a Move:\n",
    "        X.activity = X.activity.apply(lambda x: \"Move\" if (\"Move\" in x) else x)\n",
    "        \n",
    "        pre_output = self.oneHot.transform(X['activity'].values.reshape(-1,1))\n",
    "        a2 = pd.DataFrame(data = pre_output, columns = self.initial_features)\n",
    "        a2['id'] = X.id \n",
    "        act = a2.groupby('id').sum()\n",
    "        output = act.apply(lambda x: self.compute_score_by_chisquare(x, self.act_dist), axis = 1)\n",
    "        output.name = \"act_chisq\"\n",
    "        self.feature_names = output.name\n",
    "        return output.values.reshape(-1,1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b47e88c0-5c7d-4bdd-a8a3-e7e426d09f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to combine summary:\n",
    "cp_pipe = Pipeline([('cp_tx', CursorPositionTransformer())])\n",
    "wc_pipe = Pipeline([('wc_tx', WordCountTransformer())])\n",
    "tc_pipe = Pipeline([('tc_tx', TextChangeTransformer())])\n",
    "ue_pipe = Pipeline([('ue_tx', UpEventTransformer())])\n",
    "at_pipe = Pipeline([('at_tx', ActionTimeTransformer(scores = scores))])\n",
    "act_pipe = Pipeline([(\"act_tx\", ActivityTransformer(scores))])\n",
    "\n",
    "#join the pipes:\n",
    "main_pipe = FeatureUnion(transformer_list = [\n",
    "    ('cp_pipe', cp_pipe),\n",
    "    ('wc_pipe', wc_pipe),\n",
    "    ('tc_pipe', tc_pipe),\n",
    "    ('ue_pipe', ue_pipe),\n",
    "    ('at_pipe', at_pipe),\n",
    "    ('act_pipe', act_pipe)])\n",
    "\n",
    "final_pipe = Pipeline([('main_pipe', main_pipe),\n",
    "                        ('Poly', PolynomialFeatures(degree = 2, include_bias = False)),\n",
    "                        ('Scaler', StandardScaler())\n",
    "                         ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f08b297-4c38-4412-ba89-82b7af658f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_pipe.fit_transform(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fbb30ac-95b0-4ca3-83a8-be841e6d2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = main_pipe.named_transformers['cp_pipe'].named_steps['cp_tx'].index_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c64307-1421-4d48-bedb-29ed573d7183",
   "metadata": {},
   "source": [
    "# **Modeling Portion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11bc907a-4258-4d41-bc1d-5d1a4c3a73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dum main pipe\n",
    "#joblib.dump(X, \"transformed_train.pkl\")\n",
    "#joblib.dump(train_ids, \"train_ids.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c654a910-a69b-4555-b362-0aa73d78c185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2471,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = scores.values\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ce741af-9669-4dfb-955d-0c2b788d6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking , My Style Ensemble:\n",
    "class StackingModel(BaseEstimator):\n",
    "    def __init__(self, degree = 2, n_estimators = 30, max_samples = 100, random_state=11):\n",
    "        self.degree = degree\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.random_state = random_state\n",
    "        self.estimator_1 = BaggingRegressor(estimator = xgb.XGBRegressor(tree_method = \"hist\", device = 'cpu',\n",
    "                                                             random_state = self.random_state), \n",
    "                                            max_samples = self.max_samples, n_estimators = self.n_estimators, \n",
    "                                            random_state = self.random_state + 1, n_jobs = -1)\n",
    "        self.estimator_2 = BaggingRegressor(estimator = SVR(C=1000, epsilon = 0.001), \n",
    "                                            max_samples = self.max_samples, \n",
    "                                            n_estimators = self.n_estimators,\n",
    "                                            random_state = self.random_state + 2, n_jobs = -1)\n",
    "        self.estimator_3 = RandomForestRegressor()\n",
    "        self.meta_estimator = BaggingRegressor(estimator = \n",
    "                                               xgb.XGBRegressor(tree_method = \"hist\", device = 'cpu'), \n",
    "                                               max_samples = self.max_samples, n_estimators = self.n_estimators, \n",
    "                                               random_state = self.random_state + 3, n_jobs = -1)\n",
    "        # Plyfeature to increase features before passing to the meta estimator\n",
    "        self.splineTransformer= SplineTransformer(degree = 2, n_knots = 3) \n",
    "        self.standardScaler = StandardScaler()\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        print(\"StackingModel: Fitting Starts\")\n",
    "        #fit the estimators:\n",
    "        p1 = self.estimator_1.fit(X,y).predict(X)\n",
    "        print(\"StackingModel: Completed fitting for estimator_1\")\n",
    "        p2 = self.estimator_2.fit(X,y).predict(X)\n",
    "        print(\"StackingModel: Completed fitting for estimator_2\")\n",
    "        p3 = self.estimator_3.fit(X,y).predict(X)\n",
    "        print(\"StackingModel: Completed fitting for estimator_3\")\n",
    "\n",
    "        f1 = np.c_[p1, p2, p3]\n",
    "        f1 = self.splineTransformer.fit_transform(f1)\n",
    "        f1 = self.standardScaler.fit_transform(f1)\n",
    "        #fit the meta estimator:\n",
    "        self.meta_estimator.fit(f1,Y)\n",
    "        print(\"StackingModel: Completed fitting for the meta_estimator\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        p1 = self.estimator_1.predict(X)\n",
    "        print(\"StackingModel: Completed base prediction of estimator_1\")\n",
    "        p2 = self.estimator_2.predict(X)\n",
    "        print(\"StackingModel: Completed base prediction of estimator_2\")\n",
    "        p3 = self.estimator_3.predict(X)\n",
    "        print(\"StackingModel: Completed base prediction of estimator_3\")\n",
    "\n",
    "        f1 = np.c_[p1, p2, p3]\n",
    "        f1 = self.splineTransformer.transform(f1)\n",
    "        f1 = self.standardScaler.transform(f1)\n",
    "        print(\"StackingModel: Meta_estimator is predicting\")\n",
    "        output = self.meta_estimator.predict(f1)\n",
    "        return output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "736d36fe-c3b9-4964-a8b6-cdc68e5466f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking , Version 2:\n",
    "class StackingModel2(BaseEstimator):\n",
    "    def __init__(self, degree = 2, n_estimators = 30, max_samples = 100, random_state=11):\n",
    "        self.degree = degree\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.random_state = random_state\n",
    "        self.estimator_1 = BaggingRegressor(estimator = xgb.XGBRegressor(tree_method = \"hist\", device = 'cpu',\n",
    "                                                             random_state = self.random_state), \n",
    "                                            max_samples = self.max_samples, n_estimators = self.n_estimators, \n",
    "                                            random_state = self.random_state + 1, n_jobs = -1)\n",
    "        self.estimator_2 = BaggingRegressor(estimator = SVR(C=1000, epsilon = 0.001), \n",
    "                                            max_samples = self.max_samples, \n",
    "                                            n_estimators = self.n_estimators,\n",
    "                                            random_state = self.random_state + 2, n_jobs = -1)\n",
    "        self.estimator_3 = RandomForestRegressor()\n",
    "        self.meta_estimator = BaggingRegressor(estimator = \n",
    "                                               xgb.XGBRegressor(tree_method = \"hist\", device = 'cpu'), \n",
    "                                               max_samples = self.max_samples, n_estimators = self.n_estimators, \n",
    "                                               random_state = self.random_state + 3, n_jobs = -1)\n",
    "        \n",
    "        self.stackingRegressor = StackingRegressor(estimators = [('est1',self.estimator_1),\n",
    "                                                                 ('est2',self.estimator_2),\n",
    "                                                                 ('est3',self.estimator_3)], \n",
    "                                                   final_estimator = self.meta_estimator, n_jobs = -1,\n",
    "                                                  verbose = 1)\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        print(\"StackingModel2: Fitting Started\")\n",
    "        self.stackingRegressor.fit(X,y)\n",
    "        print(\"StackingModel2: Fitting Finished\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.stackingRegressor.predict(X) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6170fe1e-7a88-4d0a-a929-964ce2217b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_predictions(x):\n",
    "    score_list = np.arange(start = 0.5, stop = 6.5, step = 0.5)\n",
    "    subtracted = np.abs(score_list - x)\n",
    "    index_minimum = np.where(subtracted == np.min(subtracted))[0][0]\n",
    "    return score_list[index_minimum]\n",
    "\n",
    "\n",
    "def transform_predictions(z: np.array):\n",
    "    return np.array(list(map(adjust_predictions, z)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d99a2-1b96-42d2-aa1c-12d6a4abbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'subsample': [0.5, 0.7, 1]\n",
    "}\n",
    "model = BaggingRegressor(estimator = xgb.XGBRegressor(tree_method = \"hist\", device = 'cpu'),\n",
    "                         random_state = 11, max_samples = .4, max_features = 0.6, n_estimators = 100)\n",
    "# Create the GridSearchCV object\n",
    "grid_search_model = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da29b287-ff50-41b3-a7c6-ec853d118b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingRegressor(estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                        callbacks=None, colsample_bylevel=None,\n",
       "                                        colsample_bynode=None,\n",
       "                                        colsample_bytree=None, device=&#x27;cpu&#x27;,\n",
       "                                        early_stopping_rounds=None,\n",
       "                                        enable_categorical=False,\n",
       "                                        eval_metric=None, feature_types=None,\n",
       "                                        gamma=None, grow_policy=None,\n",
       "                                        importance_type=None,\n",
       "                                        interaction_constraints=None,\n",
       "                                        learning_rate=None, max_bin=None,\n",
       "                                        max_cat_threshold=None,\n",
       "                                        max_cat_to_onehot=None,\n",
       "                                        max_delta_step=None, max_depth=None,\n",
       "                                        max_leaves=None, min_child_weight=None,\n",
       "                                        missing=nan, monotone_constraints=None,\n",
       "                                        multi_strategy=None, n_estimators=None,\n",
       "                                        n_jobs=None, num_parallel_tree=None,\n",
       "                                        random_state=None, ...),\n",
       "                 max_features=0.6, max_samples=0.4, n_estimators=100,\n",
       "                 random_state=11)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingRegressor</label><div class=\"sk-toggleable__content\"><pre>BaggingRegressor(estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                        callbacks=None, colsample_bylevel=None,\n",
       "                                        colsample_bynode=None,\n",
       "                                        colsample_bytree=None, device=&#x27;cpu&#x27;,\n",
       "                                        early_stopping_rounds=None,\n",
       "                                        enable_categorical=False,\n",
       "                                        eval_metric=None, feature_types=None,\n",
       "                                        gamma=None, grow_policy=None,\n",
       "                                        importance_type=None,\n",
       "                                        interaction_constraints=None,\n",
       "                                        learning_rate=None, max_bin=None,\n",
       "                                        max_cat_threshold=None,\n",
       "                                        max_cat_to_onehot=None,\n",
       "                                        max_delta_step=None, max_depth=None,\n",
       "                                        max_leaves=None, min_child_weight=None,\n",
       "                                        missing=nan, monotone_constraints=None,\n",
       "                                        multi_strategy=None, n_estimators=None,\n",
       "                                        n_jobs=None, num_parallel_tree=None,\n",
       "                                        random_state=None, ...),\n",
       "                 max_features=0.6, max_samples=0.4, n_estimators=100,\n",
       "                 random_state=11)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=&#x27;cpu&#x27;, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=&#x27;cpu&#x27;, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingRegressor(estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                        callbacks=None, colsample_bylevel=None,\n",
       "                                        colsample_bynode=None,\n",
       "                                        colsample_bytree=None, device='cpu',\n",
       "                                        early_stopping_rounds=None,\n",
       "                                        enable_categorical=False,\n",
       "                                        eval_metric=None, feature_types=None,\n",
       "                                        gamma=None, grow_policy=None,\n",
       "                                        importance_type=None,\n",
       "                                        interaction_constraints=None,\n",
       "                                        learning_rate=None, max_bin=None,\n",
       "                                        max_cat_threshold=None,\n",
       "                                        max_cat_to_onehot=None,\n",
       "                                        max_delta_step=None, max_depth=None,\n",
       "                                        max_leaves=None, min_child_weight=None,\n",
       "                                        missing=nan, monotone_constraints=None,\n",
       "                                        multi_strategy=None, n_estimators=None,\n",
       "                                        n_jobs=None, num_parallel_tree=None,\n",
       "                                        random_state=None, ...),\n",
       "                 max_features=0.6, max_samples=0.4, n_estimators=100,\n",
       "                 random_state=11)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BaggingRegressor(estimator = xgb.XGBRegressor(tree_method = \"hist\", device = 'cpu'),\n",
    "                         random_state = 11, max_samples = .4, max_features = 0.6, n_estimators = 100)\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c192d838-ce43-454c-ace7-5838c2fd316f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.45276672479876817\n",
      "RMSE2: 0.4720003155243854\n"
     ]
    }
   ],
   "source": [
    "#remove below:\n",
    "predictions = model.predict(X)\n",
    "rmse = np.sqrt(mean_squared_error(predictions, Y))\n",
    "print(\"RMSE: {}\".format(rmse))\n",
    "#rmse 2 using adjusted predictions:\n",
    "predictions_2 = transform_predictions(predictions)\n",
    "rmse2 = np.sqrt(mean_squared_error(predictions_2, Y))\n",
    "print(\"RMSE2: {}\".format(rmse2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401cd910-1730-4553-a608-76fca518553f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f161a10-c47f-4357-889e-f5278efb0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction:\n",
    "X_test = final_pipe.transform(test_logs)\n",
    "test_ids = test_logs.id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45b3e5c7-c58e-4f7e-af48-4beeef749b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackingModel: Completed base prediction of estimator_1\n",
      "StackingModel: Completed base prediction of estimator_2\n",
      "StackingModel: Completed base prediction of estimator_3\n",
      "StackingModel: Meta_estimator is predicting\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "submission = pd.DataFrame({'id':test_ids, 'score':prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f040c1e-14f4-43fb-adbe-95a176a74553",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde2f9e0-daba-467f-ad2b-02f326a2319c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
