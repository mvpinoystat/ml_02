{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bef5c308-3bda-41b1-b71b-fe2f7159023a",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4c398f-e96b-4315-bed0-51a28628b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler, SplineTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#models\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa04a126-e352-42a2-b6c8-e1dee66c0c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"kaggle/input/linking-writing-processes-to-writing-quality/\"\n",
    "train_logs = pd.read_csv(input_folder + \"train_logs.csv\",delimiter = \",\",header = 0)\n",
    "train_scores = pd.read_csv(input_folder +\"train_scores.csv\", delimiter = \",\", header = 0)\n",
    "scores = pd.Series(data = train_scores['score'].values, index = train_scores['id'].values, name = 'score')\n",
    "test_logs = pd.read_csv(input_folder + \"test_logs.csv\",delimiter = \",\",header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bfd31e3-a52c-4fa9-9fad-88fc1194092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for transformer for cursor position\n",
    "class CursorPositionTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # setup the feature names\n",
    "        # self.feature_names = ['cp_sum_backstep', 'cp_n_backstep', 'cp_sum_forwardstep','cp_n_forwardstep',\n",
    "        #              'cp_change_stat', 'cp_skew_backstep', 'cp_skew_forwardstep']  \n",
    "\n",
    "        self.feature_names = ['cp_sum_backstep', 'cp_n_backstep', 'cp_sum_forwardstep','cp_n_forwardstep',\n",
    "                             'average_cp']\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        z = X.groupby('id')['cursor_position'].aggregate([self.cp_sum_backstep,self.cp_n_backstep, \n",
    "                     self.cp_sum_forwardstep, self.cp_n_forwardstep, lambda x: np.log(np.mean(x))])\n",
    "        # make a copy of participant ids:\n",
    "        self.index_ids = z.index.values\n",
    "        return z.values\n",
    "\n",
    "    def cp_sum_backstep(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return np.sum(n1[n1 < 0])\n",
    "    \n",
    "    def cp_skew_backstep(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return st.skew(n1[n1 < 0])\n",
    "    \n",
    "    def cp_n_backstep(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return np.log((n1<0).sum()+1)\n",
    "    \n",
    "    def cp_sum_forwardstep(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return np.sum(n1[n1 > 0])\n",
    "    \n",
    "    def cp_skew_forwardstep(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return st.skew(n1[n1 > 0])\n",
    "    \n",
    "    def cp_n_forwardstep(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return np.log((n1>0).sum()+1)\n",
    "    \n",
    "    def cp_change_stat(self,x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        return np.std(n1, ddof = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa1c7d61-6548-4353-a292-d7bae2300246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda wordcount transformer:\n",
    "\n",
    "# word_count feature engineering\n",
    "# Based on the graph above, we can count the number of zero changes and get the mean:\n",
    "# wc_zero_change will return the count of all non-zero steps taken by the person\n",
    "\n",
    "class WordCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def wc_non_zero_change(self, x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        n2 = np.count_nonzero(n1)\n",
    "        return n2\n",
    "    def wc_change_stat(self, x):\n",
    "        n1 = np.diff(np.log(x+1))\n",
    "        last_cutoff = n1.shape[0]-200\n",
    "        n2 = np.std(n1, ddof = 1)\n",
    "        return n2\n",
    "        \n",
    "    def transform(self, X):\n",
    "        output =  X.groupby(['id'])['word_count'].aggregate([self.wc_non_zero_change,lambda x: np.log(len(x)), \n",
    "                          lambda x: np.log(np.max(x)+1)])\n",
    "        output.columns = [\"wc_changing_nsteps\", \"wc_step_count\", \"wc_max\"]\n",
    "        self.feature_names = output.columns.values\n",
    "        self.index_ids = output.index.values\n",
    "        return output.values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7645e712-9d3e-40d4-aa02-eee5a2f18294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda textchange transformer:\n",
    "# added tc\n",
    "class TextChangeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def hasChar(self, x,character:str):\n",
    "        out = 0 \n",
    "        for strings in x:\n",
    "            if character in strings:\n",
    "                out = 1\n",
    "                break\n",
    "        return out\n",
    "    \n",
    "    def qCounter(self,c):\n",
    "        h = \" \".join(c)\n",
    "        return np.log(len(re.findall(r\" q \", h))+1)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        output = X.groupby(['id'])['text_change'].aggregate([\n",
    "            (\"tc_1\", lambda x: self.hasChar(x,character = \"?\")), \n",
    "            (\"tc_2\", lambda x: self.hasChar(x,character = \"=>\")), \n",
    "            (\"tc_3\", lambda x: self.hasChar(x,character = \"(\")), \n",
    "            (\"tc_4\", lambda x: self.hasChar(x,character = \"\\\"\")), \n",
    "            (\"tc_5\", lambda x: self.hasChar(x,character = \"-\")), \n",
    "            (\"tc_6\", lambda c: self.qCounter(c))]) \n",
    "        self.feature_names = output.columns.values\n",
    "        self.index_ids = output.index.values\n",
    "        return output.values\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2befae2-5181-4548-94e7-c9c5ac4e1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eda Text Change  Part 2\n",
    "class TextChangeTransformer2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_word_length):\n",
    "        self.max_word_length = max_word_length\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "        \n",
    "    def text_change_distribution(self, v:str):\n",
    "        distribution_container = []\n",
    "        start_flag = 1\n",
    "        word_count = 0\n",
    "        size = 0\n",
    "        for i in range(1,self.max_word_length + 2):\n",
    "            s = \"q{%s} \" % i\n",
    "            f = re.findall(s, v)\n",
    "            if(start_flag == 1):\n",
    "                word_count = len(f)\n",
    "                start_flag = 0\n",
    "            else:\n",
    "                size = word_count - len(f)\n",
    "                distribution_container.append(size)\n",
    "                word_count = len(f)\n",
    "                \n",
    "        return distribution_container\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \n",
    "        X = X.groupby('id')['text_change'].aggregate(lambda r: self.text_change_distribution(\"\".join(r)))\n",
    "        X = np.log(pd.DataFrame(np.stack(X, axis = 0), index = X.index)+1)\n",
    "        return X.values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0542b309-5aed-46c2-b33b-1c6b0931372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Up Event Variable Transformer:\n",
    "class UpEventTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "\n",
    "    \n",
    "    def find_clicked(self, x, st:str):\n",
    "        has_string = 0\n",
    "        for event in x:\n",
    "            if(event == st):\n",
    "                has_string = 1\n",
    "                break\n",
    "        return has_string\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        output = X.groupby(['id'])['up_event'].aggregate([('ue_1',lambda x: self.find_clicked(x,\"|\")),\n",
    "                                                          ('ue_2', lambda x: self.find_clicked(x,\"Shift\")),\n",
    "                                                          ('ue_3', lambda x: self.find_clicked(x,\"Tab\")),\n",
    "                                                          ])\n",
    "        self.feature_names = output.columns.values\n",
    "        self.index_ids = output.index.values\n",
    "        return output.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0acf834e-2426-429a-b6e2-db64d907af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eda action_time variable ransformer: (AT)\n",
    "\n",
    "class ActionTimeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, scores):\n",
    "        self.scores = scores\n",
    "        self.score_values = np.arange(start = 0.5, stop = 6.5, step = 0.5)\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        #Get the action time proportion or distribution per score:\n",
    "        at_init = X.groupby('id')['action_time'].aggregate([\n",
    "            ('one', lambda x: self.above_log_count(x, from_zero = 1)),\n",
    "            ('two', lambda x: self.above_log_count(x, from_zero = 2)),\n",
    "            ('three', lambda x: self.above_log_count(x, from_zero = 3)),\n",
    "            ('four', lambda x: self.above_log_count(x, from_zero = 4)),\n",
    "            ('five', lambda x: self.above_log_count(x, from_zero = 5)),\n",
    "        ])\n",
    "        \n",
    "        at_init2 = pd.merge(at_init, self.scores, left_index = True, right_index = True)\n",
    "        at2 = at_init2.groupby(by = 'score').sum()\n",
    "        self.at_proportion= at2.apply(lambda x: x/(np.sum(at2, axis = 1)))\n",
    "        return self\n",
    "        \n",
    "    def above_log_count(self, x, from_zero = 1):\n",
    "        z = np.diff(np.log(x+1))\n",
    "        z = np.abs(z)\n",
    "        if from_zero < 5:\n",
    "            count= len(list(filter(lambda q: (q>from_zero -1) and (q < from_zero), z)))\n",
    "        else:\n",
    "            count= len(list(filter(lambda q: q>=from_zero, z )))\n",
    "        return count \n",
    "        \n",
    "    def above_log_ratio(self, x, from_zero = 1):\n",
    "        z = np.diff(np.log(x+1))\n",
    "        z = np.abs(z)\n",
    "        if from_zero < 3:\n",
    "            count= len(list(filter(lambda q: (q>from_zero -1) and (q < from_zero), z)))\n",
    "        else:\n",
    "            count= len(list(filter(lambda q: q>=from_zero, z )))\n",
    "        return np.log((count+1)/len(z)) \n",
    "\n",
    "        \n",
    "    # Use chi-square to select the score of the given participant id   \n",
    "    def compute_score_by_chisquare(self, fo:pd.Series, distribution):\n",
    "        fo =fo + 1 # to remove errors for those with zero values\n",
    "        total = np.sum(fo)\n",
    "        # print(total)\n",
    "        expected_arrays = distribution * total\n",
    "        # print(expected_arrays)\n",
    "        chi_stat = []\n",
    "        for j in range(expected_arrays.shape[0]):\n",
    "            results = st.chisquare(f_obs = fo, f_exp = expected_arrays.iloc[j])\n",
    "            chi_stat.append(results[1])\n",
    "    \n",
    "        chi_stat = np.array(chi_stat)\n",
    "        # get the maximum p-value (-1) or second to the max (-2), etc\n",
    "        score_idx_1 = np.where(chi_stat == np.partition(chi_stat,-1)[-1])[0][0]\n",
    "        score_idx_2 = np.where(chi_stat == np.partition(chi_stat,-2)[-2])[0][0]\n",
    "        score_idx_3 = np.where(chi_stat == np.partition(chi_stat,-3)[-3])[0][0]\n",
    "        score_idx_4 = np.where(chi_stat == np.partition(chi_stat,-4)[-4])[0][0]\n",
    "        score_list = [\n",
    "            self.score_values[score_idx_1],\n",
    "            self.score_values[score_idx_3],\n",
    "            self.score_values[score_idx_3],\n",
    "            self.score_values[score_idx_4]]\n",
    "        \n",
    "        return np.mean(score_list)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        transform_1 = X.groupby(\"id\")['action_time'].aggregate([\n",
    "        ('at_1', lambda x: self.above_log_ratio(x, from_zero = 1)),\n",
    "        ('at_2', lambda x: self.above_log_ratio(x, from_zero = 2)),\n",
    "        ('at_3', lambda x: self.above_log_ratio(x, from_zero = 3))\n",
    "        ])\n",
    "        \n",
    "        at_init = X.groupby('id')['action_time'].aggregate([\n",
    "            ('one', lambda x: self.above_log_count(x, from_zero = 1)),\n",
    "            ('two', lambda x: self.above_log_count(x, from_zero = 2)),\n",
    "            ('three', lambda x: self.above_log_count(x, from_zero = 3)),\n",
    "            ('four', lambda x: self.above_log_count(x, from_zero = 4)),\n",
    "            ('five', lambda x: self.above_log_count(x, from_zero = 5)),\n",
    "        ])\n",
    "        transform_2 = at_init.apply(\n",
    "            lambda x: self.compute_score_by_chisquare(x, distribution = self.at_proportion),axis = 1)\n",
    "        transform_2.name = \"at_chisq\"\n",
    "        output = pd.merge(transform_1, transform_2, left_index = True, right_index = True)\n",
    "        self.feature_names = output.columns.values\n",
    "        self.index_ids = output.index.values\n",
    "        return output.values \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94f4d027-ec0f-4db3-aa93-3d398a464359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer for Activity, act:\n",
    "class ActivityTransformer(BaseEstimator, TransformerMixin):\n",
    "    oneHot: OneHotEncoder\n",
    "    scores: pd.Series\n",
    "    act_dist: pd.DataFrame\n",
    "    feature_names: np.array\n",
    "    initial_features: np.array\n",
    "    \n",
    "    def __init__(self, scores:pd.Series):\n",
    "        self.oneHot = OneHotEncoder(handle_unknown = 'ignore', categories = 'auto', sparse_output = False)\n",
    "        self.scores = scores\n",
    "        self.score_values = np.arange(start = 0.5, stop = 6.5, step = 0.5)\n",
    "        self.initial_features = np.array(['ac_Input', 'ac_Move', 'ac_NonPro', 'ac_Paste', 'ac_RemCut', 'ac_Replace'])\n",
    "        \n",
    "    def fit(self,X, y=None):\n",
    "        #Transform X labels first:\n",
    "        #Transform all with move into a Move:\n",
    "        X.activity = X.activity.apply(lambda x: \"Move\" if (\"Move\" in x) else x)\n",
    "        #Encode then get the distribution\n",
    "        self.oneHot.fit(X)\n",
    "        a1 = self.oneHot.fit_transform(X.activity.values.reshape(-1,1))\n",
    "        a2 = pd.DataFrame(data=a1, columns=self.initial_features)\n",
    "        a2['id'] = X.id.copy()\n",
    "        \n",
    "        act = a2.groupby(by = \"id\").sum()\n",
    "        act = act + 1 # to avoid expected value of zero\n",
    "        self.act = act\n",
    "        \n",
    "        # Get the distribution for each kind of score\n",
    "        # act distribution:\n",
    "        act_dist = pd.merge(act, scores, left_index = True, right_index = True)\n",
    "        act_dist = act_dist.groupby('score').sum()\n",
    "        \n",
    "        row_total = np.sum(act_dist, axis = 1)\n",
    "        self.act_dist = act_dist.apply(lambda x: x / row_total)\n",
    "            \n",
    "        return self\n",
    "\n",
    "\n",
    "    def compute_score_by_chisquare(self, fo:pd.Series, distribution):\n",
    "        fo = fo+1\n",
    "        total = np.sum(fo)\n",
    "        # print(total)\n",
    "        # add 1 to avoid expected value of zero.\n",
    "        expected_arrays = distribution * total \n",
    "        # print(expected_arrays)\n",
    "        chi_stat = []\n",
    "        for j in range(expected_arrays.shape[0]):\n",
    "            results = st.chisquare(f_obs = fo, f_exp = expected_arrays.iloc[j])\n",
    "            chi_stat.append(results[1])\n",
    "    \n",
    "        chi_stat = np.array(chi_stat)\n",
    "        # get the maximum p-value (-1) \n",
    "        score_idx_1 = np.where(chi_stat == np.partition(chi_stat,-1)[-1])[0][0]\n",
    "        \n",
    "        return self.score_values[score_idx_1]\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        #Transform X labels first:\n",
    "        #Transform all with move into a Move:\n",
    "        X.activity = X.activity.apply(lambda x: \"Move\" if (\"Move\" in x) else x)\n",
    "        \n",
    "        pre_output = self.oneHot.transform(X['activity'].values.reshape(-1,1))\n",
    "        a2 = pd.DataFrame(data = pre_output, columns = self.initial_features)\n",
    "        a2['id'] = X.id \n",
    "        act = a2.groupby('id').sum()\n",
    "        output = act.apply(lambda x: self.compute_score_by_chisquare(x, self.act_dist), axis = 1)\n",
    "        output.name = \"act_chisq\"\n",
    "        self.feature_names = output.name\n",
    "        \n",
    "        return np.c_[output.values, act.values]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83e86586-78b8-4954-a9d8-fa9a1336e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComboActivityActionTime(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        ids = X.id.unique()\n",
    "        ids_length = ids.shape[0]\n",
    "        \n",
    "        X.activity = X.activity.apply(lambda x: \"Move\" if (\"Move\" in x) else x)\n",
    "        \n",
    "        input_cols = pd.Series(data = np.ones(shape = (ids_length,), dtype = np.int64), index = ids)\n",
    "        input_cols.name = \"Input\"\n",
    "        data_container = X.loc[X.activity == 'Input'][['action_time','id']].groupby('id').agg(\"sum\")\n",
    "        for t in data_container.index.values:\n",
    "            if (data_container.loc[t]>0).item():\n",
    "                input_cols.at[t] = data_container.loc[t].item()  \n",
    "\n",
    "\n",
    "        nonproduction_cols = pd.Series(data = np.ones(shape = (ids_length,), dtype = np.int64), index = ids)\n",
    "        nonproduction_cols.name = \"Nonproduction\"\n",
    "        data_container = X.loc[X.activity == 'Nonproduction'][['action_time','id']].groupby('id').agg(\"sum\")\n",
    "        for t in data_container.index.values: \n",
    "            if (data_container.loc[t]>0).item():\n",
    "                nonproduction_cols.at[t] = data_container.loc[t].item()  \n",
    "\n",
    "        \n",
    "        move_cols = pd.Series(data = np.ones(shape = (ids_length,), dtype = np.int64), index = ids)\n",
    "        move_cols.name = \"Move\"\n",
    "        data_container = X.loc[X.activity == 'Move'][['action_time','id']].groupby('id').agg(\"sum\")\n",
    "        for t in data_container.index.values:\n",
    "            if (data_container.loc[t]>0).item():\n",
    "                move_cols.at[t] = data_container.loc[t].item()  \n",
    "        \n",
    "\n",
    "        paste_cols = pd.Series(data = np.ones(shape = (ids_length,), dtype = np.int64), index = ids)\n",
    "        paste_cols.name = \"Paste\"\n",
    "        data_container= X.loc[X.activity == 'Paste'][['action_time','id']].groupby('id').agg('sum')\n",
    "        for t in data_container.index.values: \n",
    "            if (data_container.loc[t]>0).item():\n",
    "                paste_cols.at[t] = data_container.loc[t].item()  \n",
    "\n",
    "        \n",
    "        remove_cols = pd.Series(data = np.ones(shape = (ids_length,), dtype = np.int64), index = ids)\n",
    "        remove_cols.name = \"Remove/Cut\"\n",
    "        data_container = X.loc[X.activity == 'Remove/Cut'][['action_time','id']].groupby('id').agg('sum')\n",
    "        for t in data_container.index.values: \n",
    "            if (data_container.loc[t]>0).item():\n",
    "                remove_cols.at[t] = data_container.loc[t].item()  \n",
    "\n",
    "        \n",
    "        replace_cols = pd.Series(data = np.ones(shape = (ids_length,), dtype = np.int64), index = ids)\n",
    "        replace_cols.name = \"Replace\"\n",
    "        data_container= X.loc[X.activity == 'Replace'][['action_time','id']].groupby('id').agg('sum')\n",
    "        for t in data_container.index.values: \n",
    "            if (data_container.loc[t]>0).item():\n",
    "                replace_cols.loc[t] = data_container.loc[t].item()  \n",
    "\n",
    "        n = pd.merge(input_cols, move_cols, left_index = True, right_index = True)\n",
    "        n.columns = ['Input', 'Move']\n",
    "        n = pd.merge(n,nonproduction_cols, left_index = True, right_index = True)\n",
    "        n.columns = ['Input', 'Move', 'Nonproduction']\n",
    "        n = pd.merge(n, paste_cols, left_index = True, right_index = True)\n",
    "        n.columns = ['Input', 'Move', 'Nonproduction', 'Paste']\n",
    "        n = pd.merge(n, remove_cols, left_index = True, right_index = True)\n",
    "        n.columns = ['Input', 'Move','Nonproduction', 'Paste', 'Remove/Cut']\n",
    "        n = pd.merge(n, replace_cols, left_index = True, right_index = True)\n",
    "        n.columns = ['Input', 'Move','Nonproduction','Paste', 'Remove/Cut', 'Replace']\n",
    "        n = np.log(n)\n",
    "        return n \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b47e88c0-5c7d-4bdd-a8a3-e7e426d09f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to combine summary:\n",
    "cp_pipe = Pipeline([('cp_tx', CursorPositionTransformer())])\n",
    "wc_pipe = Pipeline([('wc_tx', WordCountTransformer())])\n",
    "tc_pipe = Pipeline([('tc_tx', TextChangeTransformer())])\n",
    "tc2_pipe = Pipeline([('tc2_tx', TextChangeTransformer2(12))])\n",
    "ue_pipe = Pipeline([('ue_tx', UpEventTransformer())])\n",
    "at_pipe = Pipeline([('at_tx', ActionTimeTransformer(scores = scores))])\n",
    "act_pipe = Pipeline([(\"act_tx\", ActivityTransformer(scores))])\n",
    "combo_act_pipe = Pipeline([(\"comboact_tx\", ComboActivityActionTime())])\n",
    "\n",
    "#join the pipes:\n",
    "main_pipe = FeatureUnion(transformer_list = [\n",
    "    ('cp_pipe', cp_pipe),\n",
    "    ('wc_pipe', wc_pipe),\n",
    "    ('tc_pipe', tc_pipe),\n",
    "    ('tc2_pipe', tc2_pipe),\n",
    "    ('ue_pipe', ue_pipe),\n",
    "    ('at_pipe', at_pipe),\n",
    "    ('act_pipe', act_pipe),\n",
    "    ('combo_act_pipe', combo_act_pipe)])\n",
    "\n",
    "final_pipe = Pipeline([('main_pipe', main_pipe),\n",
    "                        ('poly', SplineTransformer(degree = 2, n_knots = 3)),\n",
    "                        ('scaler', StandardScaler())\n",
    "                         ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f08b297-4c38-4412-ba89-82b7af658f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_pipe.fit_transform(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fbb30ac-95b0-4ca3-83a8-be841e6d2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = main_pipe.named_transformers['cp_pipe'].named_steps['cp_tx'].index_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11bc907a-4258-4d41-bc1d-5d1a4c3a73e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_ids.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dum main pipe\n",
    "joblib.dump(X, \"transformed_train.pkl\")\n",
    "joblib.dump(train_ids, \"train_ids.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da29b287-ff50-41b3-a7c6-ec853d118b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2471, 184)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f161a10-c47f-4357-889e-f5278efb0e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
